{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, pickle\n",
    "import requests, os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "###################################    LOGGER            ###################################################################\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "\n",
    "# Setup the logger to log in console and file.\n",
    "logger = logging.getLogger(\"my_logger\")  # Name your logger\n",
    "if logger.hasHandlers(): logger.handlers.clear()  # Remove all existing handlers\n",
    "logger.setLevel(logging.DEBUG)  # Set the minimum log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)  # Set the level for console output\n",
    "file_handler = logging.FileHandler(\"app.log\")  # Log to a file named 'app.log'\n",
    "file_handler.setLevel(logging.ERROR)  # Log only ERROR or higher to the file\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "console_handler.setFormatter(formatter)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Example usage of the logger\n",
    "'''\n",
    "try:\n",
    "    # Simulate an error\n",
    "    1 / 0\n",
    "except ZeroDivisionError as e:\n",
    "    logger.error(\"An error occurred\", exc_info=True)  # Log with stack trace\n",
    "'''\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "###################################    PARAMETER            ###################################################################\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "\n",
    "# Parameter\n",
    "## General\n",
    "db_user = 'myuser'\n",
    "db_password = 'mypassword'\n",
    "db_host = 'localhost'\n",
    "db_port = '7005'\n",
    "db_name = 'finance_db'\n",
    "\n",
    "## DB\n",
    "schema = \"finance_data\"\n",
    "table = \"CLOSING_PRICE_DAILY\"\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "with engine.begin() as connection: connection.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {schema};\"))\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "###################################    FUNCTIONS            ###################################################################\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "\n",
    "def get_daily_closing_price_for_stock(symbol, api_key= os.getenv(\"AOI_KEY__ALPHA_VANTAGE\")):\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={api_key}'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        return response  # Return parsed JSON response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_df_from_response(response, symbol):\n",
    "    data = response.json()\n",
    "    if 'Time Series (Daily)' in data:\n",
    "        df = pd.DataFrame.from_dict(data['Time Series (Daily)'], orient='index', dtype=float)\n",
    "        df = df.rename(columns={\n",
    "            '1. open': 'Open',\n",
    "            '2. high': 'High',\n",
    "            '3. low': 'Low',\n",
    "            '4. close': 'Close',\n",
    "            '5. volume': 'Volume'\n",
    "        })\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        df.index.name = 'Date'  # Set index name\n",
    "        df.reset_index(inplace=True)  # Reset index and bring 'Date' into a column\n",
    "\n",
    "        # Ensure the Date column is in datetime format\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        df[\"Symbol\"] = symbol\n",
    "        return df\n",
    "    else:\n",
    "        logger.info(f\"Error fetching data for {symbol}: {data.get('Note', 'Unknown error')}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def delete_df_from_db(df, symbol_to_filter, engine = engine):\n",
    "    dates_to_delete = list(df[\"Date\"])  # Ensure dates are compatible with the database format\n",
    "    dates_to_delete = [str(date) for date in dates_to_delete]  # Convert dates to strings if needed\n",
    "\n",
    "    query = f'''\n",
    "        DELETE FROM \"{schema}\".\"{table}\"\n",
    "        WHERE \"Symbol\" = :symbol\n",
    "    '''\n",
    "\n",
    "    if dates_to_delete:\n",
    "        query += ' AND \"Date\" IN :dates'\n",
    "\n",
    "    # Execute the query\n",
    "    with engine.begin() as connection:\n",
    "        params = {\"symbol\": symbol_to_filter}\n",
    "        if dates_to_delete:\n",
    "            params[\"dates\"] = tuple(dates_to_delete)\n",
    "\n",
    "        delete_query = text(query)\n",
    "        result = connection.execute(delete_query, params)\n",
    "        logger.info(f\"{result.rowcount} rows deleted.\")\n",
    "\n",
    "    logger.info(f\"DF VALUES DELETED FROM DB\")\n",
    "\n",
    "def write_df_to_db(df, symbol, engine = engine):\n",
    "    delete_df_from_db(df, symbol)\n",
    "    try:\n",
    "        df.to_sql(table, engine, if_exists='append', index=False, schema=schema)\n",
    "    except Exception as e:\n",
    "        # Log the error with details\n",
    "        logging.error(f\"Failed to write data to table {schema}.{table}: {e}\")\n",
    "\n",
    "    logger.info(\"Data written to PostgreSQL successfully!\")\n",
    "\n",
    "# GET DATA. This only needs to run once. With free version, this can only run about 25 times a day due to rate limit.\n",
    "def get_source_data(list_symbols = [\"AAPL\", \"MSFT\", \"NVDA\", \"AMZN\", \"GOOGL\", \"GOOG\", \"META\", \"TSLA\", \"BRK.B\", \"AVGO\"]):\n",
    "    j_response = {}\n",
    "    for symbol in list_symbols:\n",
    "        j_response[symbol] = get_daily_closing_price_for_stock(symbol)\n",
    "\n",
    "    return j_response\n",
    "\n",
    "j_response = get_source_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dictionary to a pickle file. Ensures we have the source data.\n",
    "with open(\"j_response.pkl\", \"wb\") as file: pickle.dump(j_response, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_df = {}\n",
    "for symbol, response in j_response.items():\n",
    "    df = get_df_from_response(response, symbol)\n",
    "    write_df_to_db(df, symbol)\n",
    "    j_df[symbol] = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_df_from_db(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All stuff before was done to get data. Now we evaluate them,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query db\n",
    "query = f'SELECT * FROM \"{schema}\".\"{table}\";'\n",
    "with engine.connect() as connection: df = pd.read_sql(query, connection)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the list of symbols for which to plot\n",
    "symbols = ['AAPL', 'MSFT']\n",
    "\n",
    "# Filter the DataFrame for the selected symbols\n",
    "filtered_df = df[df['Symbol'].isin(symbols)]\n",
    "\n",
    "# Plot for each symbol\n",
    "for symbol in symbols:\n",
    "    symbol_data = filtered_df[filtered_df['Symbol'] == symbol]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot closing prices on the left y-axis\n",
    "    ax1.set_title(f\"Closing Price and Percentage Change for {symbol}\")\n",
    "    ax1.plot(symbol_data['Date'], symbol_data['Close'], label='Closing Price', color='blue')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Closing Price', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for the percentage change\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(symbol_data['Date'], symbol_data['Percent_Change'], label='Percentage Change', color='green', linestyle='--')\n",
    "    ax2.set_ylabel('Percentage Change (%)', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    # Add legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.1] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# query db\n",
    "query = f'SELECT * FROM \"{schema}\".\"{table}\";'\n",
    "with engine.connect() as connection: df = pd.read_sql(query, connection)\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Calculate daily returns for each symbol\n",
    "df['Percent_Change'] = df.groupby('Symbol')['Close'].pct_change() * 100\n",
    "\n",
    "# Pivot table to create a matrix of symbols with daily percent changes\n",
    "returns_df = df.pivot(index='Date', columns='Symbol', values='Percent_Change').dropna()\n",
    "\n",
    "# Portfolio weights (example: equal weights for all symbols)\n",
    "symbols = returns_df.columns\n",
    "weights = np.array([1 / len(symbols)] * len(symbols))  # Equal weights\n",
    "\n",
    "# Calculate portfolio returns\n",
    "portfolio_returns = returns_df.dot(weights / 100)  # Normalize weights for percentages\n",
    "\n",
    "# Monte Carlo Simulation\n",
    "num_simulations = 10000\n",
    "mean = portfolio_returns.mean()\n",
    "std_dev = portfolio_returns.std()\n",
    "\n",
    "simulated_returns = np.random.normal(mean, std_dev, num_simulations)\n",
    "var_95 = np.percentile(simulated_returns, 5)  # 5th percentile for 95% confidence level\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot histogram of simulated returns\n",
    "plt.hist(simulated_returns, bins=50, alpha=0.7, color='blue', label='Simulated Returns')\n",
    "\n",
    "# Add VaR line\n",
    "plt.axvline(var_95, color='red', linestyle='--', linewidth=2, label=f'VaR (95%): {var_95:.2f}%')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Monte Carlo Simulation of Portfolio Returns and Value at Risk (VaR)')\n",
    "plt.xlabel('Portfolio Return (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Display VaR\n",
    "print(f\"Portfolio VaR at 95% confidence level: {var_95:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
